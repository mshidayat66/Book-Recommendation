# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi - Collaborative Filtering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AYOdNXEBq1ndWJDfgf6OjoKdvsEi5sQt

# Data Understanding

## Menyiapkan semua library yang dibutuhkan
"""

import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

from google.colab import files

files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d 'arashnic/book-recommendation-dataset'
!unzip book-recommendation-dataset.zip

# load the dataset
book = pd.read_csv('Books.csv')
ratings = pd.read_csv('Ratings.csv')
users = pd.read_csv('Users.csv')

print('Jumlah buku : ', len(book.ISBN.unique()))
print('Jumlah User : ', len(users['User-ID'].unique()))
print('Jumlah user yang memberikan rating : ', len(ratings['User-ID'].unique()))
print('Jumlah buku yang memiliki rating : ', len(ratings.ISBN.unique()))

"""# Univariate Exploratory Data Analysis

## Book Variabel
"""

book.head()

"""Terdapat beberapa fitur untuk buku mulai dari `ISBN`, `Judul Buku`, `Penulis Buku`, `Tahun Terbit`, `Penerbit` dan `Cover Buku` mulai dari ukuran S sampai L"""

book.info()

"""- Terdapat 271.360 baris `data`
- Jumlah kolom sebanyak 8 `kolom`
- Terdapat 8 `tipe data` object
- Terdapat missing value pada kolom `Book-Author`, `Publisher`, `Image-URL-L`
"""

print('Jumlah buku : ', len(book.ISBN.unique()))
print('Jumlah Author : ', len(book['Book-Author'].unique()))
print('Jumlah Publisher : ', len(book.Publisher.unique()))

"""Terdapat 271.360 `buku` dengan `author` sebanyak 102.023 dan `publisher` sebanyak 16.808

## Ratings Variabel
"""

ratings.head()

ratings.info()

"""- Terdapat 1.149.780 baris `data`
- Jumlah kolom sebanyak 3 `kolom`
- Terdapat 2 `tipe data` int64
- Terdapat 1 `tipe data` object
"""

ratings.describe()

"""Rating paling rendah adalah 0 yang artinya buku belum memiliki rating akan tetapi pernah dibaca/user sudah pernah berinteraksi dengan buku dan rating paling tinggi yaitu 10"""

print('Jumlah user yang memberikan rating : ', len(ratings['User-ID'].unique()))
print('Jumlah buku yang memiliki rating : ', len(ratings.ISBN.unique()))

"""## Users Variabel"""

users.head()

users.info()

"""- Terdapat 278.858 baris `data`
- Jumlah kolom sebanyak 3 `kolom`
- Terdapat 1 `tipe data` int64
- Terdapat 1 `tipe data` object
- Terdapat 1 `tipe data` float64
- Terdapat missing value pada kolom `Age`
"""

users.describe()

print('Jumlah User : ', len(users['User-ID'].unique()))
print('Jumlah Location : ', len(users.Location.unique()))

"""# Data Preprocessing

## Menggabungkan Buku
"""

# Menggabungkan seluruh Buku
book_all = np.concatenate((
    book.ISBN.unique(),
    ratings.ISBN.unique()
))

# Mengurutkan data dan menghapus data yang sama
book_all = np.sort(np.unique(book_all))

print('Jumlah seluruh data buku berdasarkan ISBN : ', len(book_all))

all_book = pd.merge(ratings, book, on='ISBN', how='left')
all_book

"""## Menggabungkan User


"""

# Menggabungkan seluruh User-ID
user_all = np.concatenate((
    ratings['User-ID'].unique(),
    users['User-ID'].unique()
))

# Mengurutkan data dan menghapus data yang sama
user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh user : ', len(user_all))

"""# Data Preparation

## Rating

### Mengatasi Missing Value
"""

# Mengecek missing value pada dataframe ratings
ratings.isnull().sum()

"""### Mengatasi Data Duplikat"""

# Menghitung jumlah total baris yang terduplikat dalam DataFrame
total_duplicates = ratings[ratings.duplicated()].shape[0]

print(f"Jumlah total duplikat: {total_duplicates}")

# Menghapus buku yang memiliki rating 0
ratings = ratings[ratings['Book-Rating'] != 0]
ratings

# Mengubah User-ID menjadi list tanpa nilai yang sama
user_ids = ratings['User-ID'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = ratings['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

# Mapping userID ke dataframe user
ratings['user'] = ratings['User-ID'].map(user_to_user_encoded)

# Mapping placeID ke dataframe buku
ratings['book'] = ratings['ISBN'].map(book_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_book = len(book_encoded_to_book)
print(num_book)

# Mengubah rating menjadi nilai float
ratings['rating'] = ratings['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(ratings['rating'])

# Nilai maksimal rating
max_rating = max(ratings['rating'])

print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""## Buku

### Mengatasi Missing Value
"""

# Mengecek missing value pada dataframe all_book
all_book.isnull().sum()

# Membersihkan missing value dengan fungsi dropna()
all_book_clean = all_book.dropna()
all_book_clean

# Mengecek kembali missing value pada variabel all_book_clean
all_book_clean.isnull().sum()

# Mengurutkan buku berdasarkan ISBN kemudian memasukkannya ke dalam variabel fix_book
fix_book = all_book_clean.sort_values('ISBN', ascending=True)
fix_book

"""### Mengatasi Data Duplikat"""

# Membuang data duplikat pada variabel preparation
preparation = fix_book.drop_duplicates('ISBN')
preparation

# Menghapus buku yang memiliki rating 0
preparation = preparation[preparation['Book-Rating'] != 0]
preparation

# Mengecek berapa jumlah fix_book
len(fix_book.ISBN.unique())

# Mengonversi data series ‘ISBN’ menjadi dalam bentuk list
book_id = preparation['ISBN'].tolist()

# Mengonversi data series ‘Book-Title’ menjadi dalam bentuk list
book_name = preparation['Book-Title'].tolist()

# Mengonversi data series ‘Book-Author’ menjadi dalam bentuk list
book_author = preparation['Book-Author'].tolist()

# Mengonversi data series ‘Publisher’ menjadi dalam bentuk list
book_publisher = preparation['Publisher'].tolist()

print(len(book_id))
print(len(book_name))
print(len(book_author))
print(len(book_publisher))

# Membuat dictionary untuk data ‘book_id’, ‘book_name’, ‘book_author’ dan ‘publisher’
book_new = pd.DataFrame({
    'id': book_id,
    'book_name': book_name,
    'book_author': book_author,
    'publisher': book_publisher,
})
book_new

"""## Membagi Data untuk Training dan Validasi"""

# Mengacak dataset
df = ratings.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan buku menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# Model Development

## Proses Training
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings buku
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding buku bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 5,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Mendapatkan Rekomendasi Buku"""

book_df = book_new
df = pd.read_csv('Ratings.csv')

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_read_by_user = df[df['User-ID'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_read = book_df[~book_df['id'].isin(book_read_by_user.ISBN.values)]['id']
book_not_read = list(
    set(book_not_read)
    .intersection(set(book_to_book_encoded.keys()))
)

book_not_read = [[book_to_book_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_read[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_read_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = book_df[book_df['id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.book_name, 'by', row.book_author, 'publish by', row.publisher)

print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

recommended_book = book_df[book_df['id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.book_name, 'by', row.book_author, 'publish by', row.publisher)

